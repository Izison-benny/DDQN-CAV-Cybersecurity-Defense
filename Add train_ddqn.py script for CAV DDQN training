train_ddqn_code = """\
# train_ddqn.py

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from ddqn_agent import DDQNAgent
from cav_env_v3 import CAVExecutionEnv
import os

# ======================
# Initialize Environment
# ======================
env = CAVExecutionEnv(max_steps=300)
agent = DDQNAgent(
    obs_dim=env.observation_space.shape[0],
    action_dims=env.action_space.nvec.tolist()
)

episodes = 3000
reward_log, epsilon_log = [], []
switch_log, intrusion_log, qos_log = [], [], []
reward_components_log = []
security_log, security_level_log = [], []
zero_day_log, defense_log = [], []

total_intrusions = 0
total_zero_days = 0
total_steps = 0
total_attack_attempts = 0

print(f" Starting training for {episodes} episodes...")
print(f"Observation space: {env.observation_space.shape[0]} | Action space: {env.action_space.nvec}")

for ep in range(episodes):
    state = env.reset()
    episode_reward = 0
    done = False
    episode_intrusions = 0
    episode_zero_days = 0
    episode_qos = 0
    step_count = 0
    episode_attack_attempts = 0
    episode_defended = 0

    while not done:
        action_idx = agent.act(state)
        action = np.unravel_index(action_idx, env.action_space.nvec)
        next_state, reward, done, info = env.step(list(action))

        if info.get('attack_status') == 1:
            episode_intrusions += 1
            if info.get('attack_type') == 'zero-day':
                episode_zero_days += 1
        elif info.get('attack_type') == 'defended':
            episode_defended += 1
        if info.get('attack_type') != 'none':
            episode_attack_attempts += 1

        episode_qos = info.get('qos', 0)
        step_count += 1
        agent.remember(state, action_idx, reward, next_state, done)
        state = next_state
        episode_reward += reward

        if len(agent.memory) > agent.batch_size:
            agent.replay()

        if "reward_components" in info:
            reward_components_log.append(info["reward_components"])

    total_intrusions += episode_intrusions
    total_zero_days += episode_zero_days
    total_attack_attempts += episode_attack_attempts
    total_steps += step_count

    episode_security_rate = episode_defended / episode_attack_attempts if episode_attack_attempts > 0 else 1.0
    security_log.append(episode_security_rate)
    security_level_log.append(info.get('security_level', 'NORMAL'))

    reward_log.append(episode_reward)
    epsilon_log.append(agent.epsilon)
    switch_log.append(info.get("switches", 0))
    intrusion_log.append(episode_intrusions)
    qos_log.append(episode_qos)
    zero_day_log.append(episode_zero_days)
    defense_log.append(episode_defended)

    agent.update_epsilon()
    cumulative_avg_reward = np.mean(reward_log)

    if (ep + 1) % 100 == 0 or (ep + 1) == episodes:
        avg_reward = np.mean(reward_log[-100:])
        avg_security = np.mean(security_log[-100:])
        avg_qos = np.mean(qos_log[-100:])
        avg_switches = np.mean(switch_log[-100:])
        security_level = max(set(security_level_log[-100:]), key=security_level_log[-100:].count)

        print(f"Episode {ep+1}/{episodes} | Reward: {avg_reward:.2f} | Cumulative Avg: {cumulative_avg_reward:.2f} | "
              f"Security: {avg_security:.2%} | QoS: {avg_qos:.2%} | "
              f"Switches: {avg_switches:.1f} | "
              f"Security Level: {security_level}")

# =======================================
# Training plot Metrics if applicable
# ========================================

# ======================
# Save the Final Model
# ======================
agent.save("ddqn_model_final.pth")


# ======================
# Print Final Summary
# ======================
total_reward = sum(reward_log)
avg_qos = np.mean(qos_log)
avg_switches = np.mean(switch_log)
security_rate = (total_attack_attempts - total_intrusions) / total_attack_attempts if total_attack_attempts > 0 else 1.0
zday_rate = total_zero_days / total_intrusions if total_intrusions > 0 else 0
mtbi = total_steps / total_intrusions if total_intrusions > 0 else float('inf')
avg_security_level = max(set(security_level_log), key=security_level_log.count)

print("\\n" + "="*30 + " FINAL PERFORMANCE " + "="*30)
print(f"→ Total Reward:        {total_reward:>10.1f}")
print(f"→ Security Rate:       {security_rate:>10.2%} ({total_attack_attempts} attacks)")
print(f"→ QoS Uptime:          {avg_qos:>10.2%}")
print(f"→ Intrusions:          {total_intrusions:>10} ({(total_intrusions/total_steps):.2%})")
print(f"→ Zero-Day Intrusions: {total_zero_days:>10} ({zday_rate:.1%})")
print(f"→ Defended Attacks:    {total_attack_attempts - total_intrusions:>10}")
print(f"→ MTBI:                {mtbi:>10.1f} steps")
print(f"→ Avg Switches:        {avg_switches:>10.1f}/episode")
print(f"→ Final Epsilon:       {agent.epsilon:>10.4f}")
print(f"→ Security Level:      {avg_security_level:>10}")
print(f"→ Total Steps:         {total_steps:>10}")
print("="*80)

# ======================
# Save Logs to CSV
# ======================
os.makedirs('results', exist_ok=True)

ddqn_df = pd.DataFrame({
    'episode': list(range(1, episodes+1)),
    'reward': reward_log,
    'qos_rate': qos_log,
    'intrusions': intrusion_log,
    'zero_day_intrusions': zero_day_log,
    'defended': defense_log,
    'switches': switch_log,
    'security_rate': security_log,
    'security_level': security_level_log,
    'epsilon': epsilon_log,
    'cumulative_avg_reward': pd.Series(reward_log).expanding().mean()
})

ddqn_df.to_csv("results/ddqn_metrics.csv", index=False)
print(" DDQN metrics saved to results/ddqn_metrics.csv")

if reward_components_log:
    pd.DataFrame(reward_components_log).to_csv("results/reward_components_log.csv", index=False)
    print(" Reward components saved to results/reward_components_log.csv")

print(" Training complete.")
